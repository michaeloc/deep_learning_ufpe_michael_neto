{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from tqdm import tqdm\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_a = pd.read_csv('db/dataset_entidades_actor_2.csv')\n",
    "data_c = pd.read_csv('db/dataset_entidades_company_2.csv')\n",
    "data_m = pd.read_csv('db/dataset_entidades_musical_artist_2.csv')\n",
    "data_p = pd.read_csv('db/dataset_entidades_politican_2.csv')\n",
    "data_s = pd.read_csv('db/dataset_entidades_soccer_player_2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat([data_a,data_c,data_m,data_p,data_s])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Anthony_Eustrel</td>\n",
       "      <td>['Person', 'Actor', 'Agent', 'Artist']</td>\n",
       "      <td>Actor</td>\n",
       "      <td>Anthony Eustrel (October 12, 1902-July 2, 1979...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Billy_Dee</td>\n",
       "      <td>['Person', 'Actor', 'AdultActor', 'Agent', 'Ar...</td>\n",
       "      <td>AdultActor</td>\n",
       "      <td>Not to be confused with Billy Dee Williams or ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Sid_Lucero</td>\n",
       "      <td>['Person', 'Actor', 'Agent', 'Artist']</td>\n",
       "      <td>Actor</td>\n",
       "      <td>Timothy Mark Pimentel Eigenmann, better known ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Jacques_Balutin</td>\n",
       "      <td>['Person', 'Actor', 'Agent', 'Artist']</td>\n",
       "      <td>Actor</td>\n",
       "      <td>Jacques Balutin is a French actor.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Austin_Leigh</td>\n",
       "      <td>['Person', 'Actor', 'Agent', 'Artist']</td>\n",
       "      <td>Actor</td>\n",
       "      <td>Austin Leigh was a British stage and film actor.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            index  \\\n",
       "0           0  Anthony_Eustrel   \n",
       "1           1        Billy_Dee   \n",
       "2           2       Sid_Lucero   \n",
       "3           3  Jacques_Balutin   \n",
       "4           4     Austin_Leigh   \n",
       "\n",
       "                                                   0           1  \\\n",
       "0             ['Person', 'Actor', 'Agent', 'Artist']       Actor   \n",
       "1  ['Person', 'Actor', 'AdultActor', 'Agent', 'Ar...  AdultActor   \n",
       "2             ['Person', 'Actor', 'Agent', 'Artist']       Actor   \n",
       "3             ['Person', 'Actor', 'Agent', 'Artist']       Actor   \n",
       "4             ['Person', 'Actor', 'Agent', 'Artist']       Actor   \n",
       "\n",
       "                                                   2  \n",
       "0  Anthony Eustrel (October 12, 1902-July 2, 1979...  \n",
       "1  Not to be confused with Billy Dee Williams or ...  \n",
       "2  Timothy Mark Pimentel Eigenmann, better known ...  \n",
       "3                 Jacques Balutin is a French actor.  \n",
       "4   Austin Leigh was a British stage and film actor.  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_data = all_data.groupby(\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 116/116 [00:01<00:00, 97.06it/s] \n"
     ]
    }
   ],
   "source": [
    "## Delete classes with few examples (<300)\n",
    "for item in tqdm(group_data):\n",
    "    target = item[0]\n",
    "    if len(all_data[all_data['1']==target])<300:\n",
    "        all_data.drop(all_data[all_data['1']==target].index,inplace=True )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_word_sequence(texts):\n",
    "    abstract_sentences = list()\n",
    "    for text in tqdm(texts):\n",
    "        abstract_sentences.append(text_to_word_sequence(str(text)))\n",
    "    return abstract_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = all_data.iloc[:,4].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = all_data.iloc[:,3].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['Actor', 'AdultActor', 'Company', 'Congressman', 'Governor',\n",
       "        'Mayor', 'MemberOfParliament', 'MusicalArtist', 'Politician',\n",
       "        'President', 'PrimeMinister', 'SoccerManager', 'SoccerPlayer',\n",
       "        'VoiceActor'], dtype=object), (29033,))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(labels), labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 29033/29033 [00:00<00:00, 29899.38it/s]\n"
     ]
    }
   ],
   "source": [
    "abstract_sentences = text_word_sequence(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_text(examples):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(examples)\n",
    "    sequences = tokenizer.texts_to_sequences(examples)\n",
    "    return (sequences, tokenizer)\n",
    "sequences, tokenizer = tokenize_text(abstract_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_sentences_number = sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16373237, 23699860)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = word2vec.Word2Vec(size=200, sg=1, workers=10)\n",
    "\n",
    "model.build_vocab(sentences=abstract_sentences)\n",
    "\n",
    "model.train(sentences=abstract_sentences, epochs=20, total_examples=len(abstract_sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec_dbpedia_size=200.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/83123 [00:00<?, ?it/s]/home/mobility/anaconda/envs/michael_env/lib/python3.6/site-packages/ipykernel_launcher.py:4: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "  after removing the cwd from sys.path.\n",
      "/home/mobility/anaconda/envs/michael_env/lib/python3.6/site-packages/ipykernel_launcher.py:5: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"\n",
      "100%|██████████| 83123/83123 [00:00<00:00, 231038.37it/s]\n"
     ]
    }
   ],
   "source": [
    "quantity_of_embeddings = len(model.wv.vocab)\n",
    "embedding_matrix = np.zeros((len(tokenizer.word_index), 200))\n",
    "for word, i in tqdm(tokenizer.word_index.items()):\n",
    "    if word in model and i < quantity_of_embeddings:\n",
    "        embedding_matrix[i] = model[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sequences_pad = pad_sequences(abstract_sentences_number,maxlen=100)\n",
    "\n",
    "x_train,x_test,y_train,y_test = train_test_split(sequences_pad,y,test_size=0.10, random_state=42)\n",
    "\n",
    "x_train,x_val,y_train,y_val = train_test_split(x_train,y_train, test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Embedding,Input, Conv1D, MaxPooling1D, Dense,merge,Flatten, Dropout, GlobalMaxPool1D, Concatenate\n",
    "from keras.initializers import random_uniform\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Constants\n",
    "channels = 100\n",
    "size_feature_map = 3\n",
    "size_feature_map_2 = 4\n",
    "size_feature_map_3 = 5\n",
    "dropout = 0.25\n",
    "dense1 = 50\n",
    "dense2 = 100\n",
    "embedding_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequence_input (InputLayer)  (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 100, 200)          16624600  \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 200)          0         \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 100, 100)          60100     \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 100, 100)          40100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 100, 100)          0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling1d_1 (Glob (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 100)               10100     \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 14)                714       \n",
      "=================================================================\n",
      "Total params: 16,740,664\n",
      "Trainable params: 16,740,664\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20903 samples, validate on 5226 samples\n",
      "Epoch 1/50\n",
      "  320/20903 [..............................] - ETA: 2:12 - loss: 2.5725 - acc: 0.1562"
     ]
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(tokenizer.word_index),\n",
    "                            200,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=True)\n",
    "\n",
    "sequence_input = Input(shape=(embedding_size,), dtype='int32', name='sequence_input')\n",
    "\n",
    "embedded_sequences = embedding_layer(sequence_input)\n",
    "embedded_sequences = Dropout(dropout) (embedded_sequences)\n",
    "x = Conv1D(channels, size_feature_map, activation='relu',padding='SAME')(embedded_sequences)\n",
    "x = Conv1D(channels, size_feature_map_2,activation='relu',padding='SAME')(x)\n",
    "x = Dropout(dropout)(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "# x = Flatten()(x)\n",
    "\n",
    "fc1 = Dense(dense2, activation='relu', name='fc1')(x)\n",
    "fc1 = Dropout(dropout)(fc1)\n",
    "fc2 = Dense(dense1, activation='relu', name='fc2')(fc1)\n",
    "preds = Dense(14, activation='softmax')(fc2)\n",
    "model_cnn = Model(sequence_input, preds)\n",
    "model_cnn.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['acc'])\n",
    "\n",
    "model_cnn.summary()\n",
    "checkpoint = ModelCheckpoint('weights-best-model1-dbpedia.hdf5', monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_list = [checkpoint]\n",
    "model_cnn.fit(x_train, y_train, batch_size=64, epochs=50,validation_data=(x_val,y_val),verbose=1,callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cnn.load_weights('weights-best-model3.hdf5')\n",
    "model_cnn.evaluate([x_test,x_test_rdf], y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = model.predict(x_test, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
